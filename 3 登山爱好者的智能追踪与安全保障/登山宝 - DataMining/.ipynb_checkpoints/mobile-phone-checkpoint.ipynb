{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 作品背景阐释\n",
    "\n",
    "智能手机和以智能手机为基础的生物识别使用日常生活活动是非常具有价值与意义的。而在众多的户外运动中，登山运动深受广大户外运动爱好者追捧，克服重重困难登上山顶的那一刻，兴奋与刺激溢于言表。然而近年来，登山队迷路、队员走失、山内信号微弱导致无法对外求援等问题带来的意外伤害事故时有发生。如果可以帮助登山爱好者在登山前做好智能追踪等安全保障工作，户外运动就会更加无忧、快乐——智能追踪手环、遇险自动报警装置、偏离路线智能提醒、装备不合格智能预警……\n",
    "\n",
    "# 作品说明\n",
    "本项目利用智能手机陀螺仪传感器的数据，对常见的两种传感器数据组织形式进行了建模，并搭建了高效准确、自动化、端到端的机器学习模型，能够有效的对人类活动进行识别与预测，保障登山爱好者的安全，对路径进行智能追踪与安全保障，让我们成为登山爱好者的后盾\n",
    "\n",
    "## 创新性\n",
    "本项目使用非常前沿的Permutation特征重要性技术对海量特征进行筛选，并构建多模型进一步确保了算法稳定性。在神经网络中，我们使用了Lookahead技术，能够有效找到更加光滑的梯度鞍点，并使用mish激活函数，使网络学习的更深更好\n",
    "\n",
    "## 完成度\n",
    "本项目将现有可能的数据输入形式做了汇总，并构建了端到端的易用准确模型，对人类活动数据进行了非常完善的建模与评估，完成了从自动化的特征工程到自动化的特征筛选，并可持久化存储了模型权重，方便任意时刻对类似的数据输入进行推理\n",
    "\n",
    "## 实用性\n",
    "本项目采取一键式训练方案，即无需人工进行特征工程构造，我们使用大量前沿有效的算法，将学术界优雅的解决方案带到工业界，减少先验知识的构造，更加专业的去分析用户轨迹\n",
    "\n",
    "# 商业前景\n",
    "本项目在`B端`，不仅可以利用在登山爱好者的安全保障，更可以使用在各式各样的推荐场景，对用户行为的判别可以使得推荐更准确，更符合用户需求，能够有效为推荐模型提供更为丰富的决策依据。在`C端`，可以绑定在AMS的服务器上，以服务（外设）的形式出售给登山爱好者，为其提供安全保障。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 环境依赖\n",
    "```\n",
    "tensorboard==2.2.2\n",
    "tensorboard-plugin-wit==1.6.0.post3\n",
    "tensorflow-addons==0.11.2\n",
    "tensorflow-estimator==2.2.0\n",
    "tensorflow-gpu==2.2.0\n",
    "tensorflow-probability==0.10.0\n",
    "scikit-learn==0.23.1\n",
    "scipy==1.4.1\n",
    "numpy==1.18.5\n",
    "pandas==1.0.4\n",
    "eli5==0.10.1\n",
    "lightgbm==3.0.0\n",
    "tqdm==4.46.1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T06:17:46.814050Z",
     "start_time": "2020-09-16T06:17:43.005770Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.metrics.scorer module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.feature_selection.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.feature_selection. Anything that cannot be imported from sklearn.feature_selection is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# 包引入\n",
    "import warnings\n",
    "import os\n",
    "import gc\n",
    "import eli5\n",
    "from tqdm import tqdm\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from eli5.sklearn import PermutationImportance\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import *\n",
    "from tensorflow.keras.layers import *\n",
    "from scipy.signal import resample\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "import lightgbm as lgb\n",
    "from tensorflow_addons.activations import mish, gelu\n",
    "from tensorflow_addons.callbacks import TQDMProgressBar\n",
    "from tensorflow_addons.metrics import F1Score\n",
    "from tensorflow_addons.layers import WeightNormalization\n",
    "from tensorflow_addons.optimizers import Lookahead, AdamW, RectifiedAdam\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "tfa.options.TF_ADDONS_PY_OPS = True\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"2\"\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 两种建模方式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 针对已经提取好的特征维表，我们使用LightGBM、DNN模型，并测试特征筛选、模型集成的效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T06:17:47.627259Z",
     "start_time": "2020-09-16T06:17:46.815395Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10299, 563), 30)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"./zhin1768/train.csv\")\n",
    "test = pd.read_csv(\"./zhin1768/test.csv\")\n",
    "train.shape, test.shape\n",
    "\n",
    "df = pd.concat([train, test], ignore_index=True)\n",
    "df.shape, df['subject'].nunique()  # 拼接训练集测试集、并输出总人数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T06:17:48.403952Z",
     "start_time": "2020-09-16T06:17:47.629096Z"
    }
   },
   "outputs": [],
   "source": [
    "# 将label映射成int\n",
    "\n",
    "label_hash = LabelEncoder()\n",
    "df['Activity'] = label_hash.fit_transform(df['Activity'])\n",
    "\n",
    "# 将数值特征标准化\n",
    "feature_name = [i for i in df.columns if i not in ['subject', 'Activity']]\n",
    "df[feature_name] = StandardScaler().fit_transform(df[feature_name].fillna(0)) # 缺失值按0填充\n",
    "\n",
    "target = 'Activity'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T06:17:48.413432Z",
     "start_time": "2020-09-16T06:17:48.405704Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7352, 563), (2947, 563))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 传入处理后的训练集与测试集\n",
    "train = df[:len(train)]\n",
    "test = df[len(train):].reset_index(drop=True)\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-16T06:17:51.240Z"
    }
   },
   "outputs": [],
   "source": [
    "# 我们使用大名鼎鼎的Permutation Importance做特征筛选\n",
    "\n",
    "max_features = 500\n",
    "show_featuers = 30\n",
    "lr = LogisticRegression(C=0.1, n_jobs=20)\n",
    "lr.fit(train[feature_name], train[target])\n",
    "\n",
    "perm = PermutationImportance(lr, scoring='f1_macro', random_state=2020).\\\n",
    "                             fit(train[feature_name], train[target])\n",
    "\n",
    "fi = pd.DataFrame()\n",
    "fi['feature_name'] = feature_name\n",
    "fi['score'] = perm.feature_importances_\n",
    "select_feature_name = fi.sort_values(by=['score'],ascending=False)[:max_features]['feature_name'].values\n",
    "\n",
    "eli5.show_weights(perm, feature_names = feature_name, top=show_featuers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-16T06:17:51.741Z"
    }
   },
   "outputs": [],
   "source": [
    "def lgb_model(train, target, test, kfolds=5, drop_col=['label']):\n",
    "    feats = [f for f in train.columns if f not in drop_col]\n",
    "    print('Current num of features:', len(feats))\n",
    "    folds = StratifiedKFold(n_splits=kfolds, shuffle=True, random_state=2020)\n",
    "    oof_probs = np.zeros((train.shape[0], len(label_hash.classes_)))\n",
    "    output_preds = np.zeros((test.shape[0], len(label_hash.classes_)))\n",
    "    offline_score = []\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    parameters = {\n",
    "        'learning_rate': 0.05,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'multiclass',\n",
    "        'num_class' : len(label_hash.classes_),\n",
    "        'metric': 'multi_error',\n",
    "        'num_leaves': 128,\n",
    "        'feature_fraction': 0.3,\n",
    "        'bagging_fraction': 0.3,\n",
    "        'min_data_in_leaf': 10,\n",
    "        'verbose': -1,\n",
    "        'nthread': 30,\n",
    "    }\n",
    "\n",
    "    BOOST_ROUND = 100000\n",
    "    ES = 300\n",
    "    VE = 300\n",
    "    \n",
    "    for i, (train_index, test_index) in enumerate(folds.split(train, target)):\n",
    "        train_y, test_y = target[train_index], target[test_index]\n",
    "        train_X, test_X = train[feats].iloc[train_index, :], train[feats].iloc[test_index, :]\n",
    "\n",
    "        dtrain = lgb.Dataset(train_X.values,\n",
    "                             label=train_y)\n",
    "        dval = lgb.Dataset(test_X.values,\n",
    "                           label=test_y)\n",
    "        lgb_model = lgb.train(\n",
    "                parameters,\n",
    "                dtrain,\n",
    "                num_boost_round=BOOST_ROUND,\n",
    "                valid_sets=[dval],\n",
    "                early_stopping_rounds=ES,\n",
    "                verbose_eval=VE,\n",
    "        )\n",
    "        oof_probs[test_index] = lgb_model.predict(test_X[feats].values, num_iteration=lgb_model.best_iteration)\n",
    "        offline_score.append(lgb_model.best_score['valid_0']['multi_error'])\n",
    "        output_preds += lgb_model.predict(test[feats].values, num_iteration=lgb_model.best_iteration) / folds.n_splits\n",
    "        print(offline_score)\n",
    "        # feature importance\n",
    "        fold_importance_df = pd.DataFrame()\n",
    "        fold_importance_df[\"feature\"] = feats\n",
    "        fold_importance_df[\"importance\"] = lgb_model.feature_importance(importance_type='gain')\n",
    "        fold_importance_df[\"fold\"] = i + 1\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    print('OOF-MEAN-ACC:%.6f, OOF-STD-ACC:%.6f' % (1 - np.mean(offline_score), np.std(offline_score)))\n",
    "    print('feature importance:')\n",
    "    print(feature_importance_df.groupby(['feature'])['importance'].mean().sort_values(ascending=False).head(30))\n",
    "\n",
    "    return output_preds, oof_probs, np.mean(offline_score), feature_importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-16T06:17:52.071Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_model(len_fi):\n",
    "    inp = Input(len_fi, )\n",
    "    x = Dropout(0.5)(inp)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = WeightNormalization(Dense(2048, activation=mish))(x)\n",
    "    \n",
    "    x = Dropout(0.4)(inp)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = WeightNormalization(Dense(1024, activation=mish))(x)\n",
    "    \n",
    "    x = Dropout(0.3)(inp)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = WeightNormalization(Dense(512, activation=mish))(x)\n",
    "    \n",
    "    output = WeightNormalization(Dense(len(label_hash.classes_), activation='softmax'))(x)\n",
    "    \n",
    "    model = Model([inp], output)\n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy',\n",
    "        optimizer=Lookahead(AdamW(weight_decay=1e-5, beta_1=0.9, beta_2=0.999, learning_rate=1e-3),\n",
    "                            sync_period=5),\n",
    "        metrics=[F1Score(num_classes=len(label_hash.classes_), average='macro', threshold=0.5),\n",
    "                 'acc']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def dnn_model(train, target, test, kfolds=5, drop_col=['label']):\n",
    "    feats = [f for f in train.columns if f not in drop_col]\n",
    "    folds = KFold(n_splits=kfolds, shuffle=True, random_state=2020)\n",
    "    oof_probs = np.zeros((train.shape[0], len(label_hash.classes_)))\n",
    "    output_preds = np.zeros((test.shape[0], len(label_hash.classes_)))\n",
    "    \n",
    "    try:\n",
    "        os.system(\"mkdir ./dnn_weights\")\n",
    "    except:\n",
    "        print(\"MKDIR FINISHED...\")\n",
    "        \n",
    "    ES = 20\n",
    "    EPOCHS = 2000\n",
    "    BATCH_SIZE = 128\n",
    "    target = to_categorical(target, num_classes=len(label_hash.classes_))\n",
    "    \n",
    "    for i, (train_index, test_index) in enumerate(folds.split(train, target)):\n",
    "        train_y, test_y = target[train_index], target[test_index]\n",
    "        train_X, test_X = train[feats].iloc[train_index, :], train[feats].iloc[test_index, :]\n",
    "\n",
    "        model = build_model(len(feats))\n",
    "        early_stopping = EarlyStopping(monitor='val_acc', mode='max', verbose=1, patience=ES)\n",
    "        plateau = ReduceLROnPlateau(monitor=\"val_acc\", verbose=0, mode='max', factor=0.1, patience=ES//2)\n",
    "        checkpoint = ModelCheckpoint('./dnn_weights/' + str(i) + '.hdf5', monitor='val_acc', \n",
    "                                         verbose=0, save_best_only=True, mode='max',save_weights_only=True)\n",
    "        tqdmbar = TQDMProgressBar(show_epoch_progress=False)\n",
    "        \n",
    "        model.fit(\n",
    "            train_X, train_y,\n",
    "            batch_size = BATCH_SIZE,\n",
    "            epochs = EPOCHS,\n",
    "            verbose=0,\n",
    "            validation_data = (test_X, test_y),\n",
    "            shuffle = True,\n",
    "            callbacks=[early_stopping, plateau, checkpoint, tqdmbar],\n",
    "        )\n",
    "        \n",
    "        model.load_weights('./dnn_weights/' + str(i) + '.hdf5')\n",
    "        oof_probs[test_index] = model.predict(test_X[feats].values, batch_size=BATCH_SIZE*4)\n",
    "        output_preds += model.predict(test[feats].values, batch_size=BATCH_SIZE*4) / folds.n_splits\n",
    "        \n",
    "        del model; gc.collect()\n",
    "        K.clear_session()\n",
    "        \n",
    "    return output_preds, oof_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-16T06:17:52.347Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lgb_preds, lgb_oof, lgb_score, fi = lgb_model(train=train, \n",
    "                                              target=train[target].values, \n",
    "                                              test=test, \n",
    "                                              kfolds=5, drop_col=['subject','Activity'])\n",
    "\n",
    "dnn_preds, dnn_oof = dnn_model(train=train, \n",
    "                               target=train[target].values, \n",
    "                               test=test, \n",
    "                               kfolds=5, \n",
    "                               drop_col=['subject','Activity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-16T06:17:53.225Z"
    }
   },
   "outputs": [],
   "source": [
    "# 我们使用简单的线性加权来查看分数\n",
    "\n",
    "average = accuracy_score(train[target], np.argmax(dnn_oof+lgb_oof, axis=1))\n",
    "lgb_score = accuracy_score(train[target], np.argmax(lgb_oof, axis=1))\n",
    "dnn_score = accuracy_score(train[target], np.argmax(dnn_oof, axis=1))\n",
    "\n",
    "print(\"Average {} \\nLGB {} \\nDNN {}\".format(average, lgb_score, dnn_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 根据原始陀螺仪数据进行自动化的建模"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-16T06:17:56.416Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"./xinwang/sensor_train.csv\")\n",
    "test = pd.read_csv(\"./xinwang/sensor_test.csv\")\n",
    "test['fragment_id'] += 10000 \n",
    "\n",
    "df = pd.concat([train, test], ignore_index=True)\n",
    "label = train[['fragment_id','behavior_id']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "feature_name = [i for i in df.columns if i not in ['fragment_id','behavior_id','time_point']]\n",
    "\n",
    "df[feature_name] = StandardScaler().fit_transform(df[feature_name].fillna(0).replace([np.inf, -np.inf], 0))\n",
    "train = df[df['fragment_id'].isin(train['fragment_id'])].reset_index(drop=True)\n",
    "test = df[df['fragment_id'].isin(test['fragment_id'])].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-16T06:17:56.873Z"
    }
   },
   "outputs": [],
   "source": [
    "maxlen = 60\n",
    "\n",
    "x = np.zeros((train['fragment_id'].nunique(), maxlen, len(feature_name)))\n",
    "t = np.zeros((test['fragment_id'].nunique(), maxlen, len(feature_name)))\n",
    "for i in tqdm(range(train['fragment_id'].nunique())):\n",
    "    tmp = train[train.fragment_id == i][:maxlen]\n",
    "    x[i,:,:] = resample(tmp[feature_name], maxlen, np.array(tmp.time_point))[0]\n",
    "for i in tqdm(range(test['fragment_id'].nunique())):\n",
    "    tmp = test[test.fragment_id == i + 10000][:maxlen]\n",
    "    t[i,:,:] = resample(tmp[feature_name], maxlen, np.array(tmp.time_point))[0]\n",
    "    \n",
    "y = train.groupby('fragment_id')['behavior_id'].min()\n",
    "class_num = train['behavior_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-16T06:17:58.817Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    input = Input(shape=(maxlen, len(feature_name)))\n",
    "\n",
    "    x1 = Conv1D(256, kernel_size=4, activation=mish, padding='same')(input)\n",
    "    x2 = Conv1D(256, kernel_size=3, activation=mish, padding='same')(x1)\n",
    "    x2 = Add()([x1, x2])\n",
    "    x3 = Conv1D(128, kernel_size=2, activation=mish, padding='same')(x2)\n",
    "    x4 = Conv1D(128, kernel_size=2, activation=mish, padding='same')(x3)\n",
    "    x4 = Add()([x3, x4])\n",
    "    x5 = Conv1D(128, kernel_size=2, activation=mish, padding='same')(x4)\n",
    "    x5 = Add()([x4, x5])\n",
    "    x6 = Conv1D(128, kernel_size=2, activation=mish, padding='same')(x5)\n",
    "    x6 = Add()([x5, x6])\n",
    "    x7 = Conv1D(128, kernel_size=2, activation=mish, padding='same')(x6)\n",
    "    x7 = Add()([x6, x7])\n",
    "    x8 = Conv1D(128, kernel_size=2, activation=mish, padding='same')(x7)\n",
    "    x8 = Add()([x7, x8])\n",
    "\n",
    "    Y1 = Lambda(lambda temp: K.max(temp, axis=1))(x1)\n",
    "    Y2 = Lambda(lambda temp: K.max(temp, axis=1))(x2)\n",
    "    Y3 = Lambda(lambda temp: K.max(temp, axis=1))(x3)\n",
    "    Y4 = Lambda(lambda temp: K.max(temp, axis=1))(x4)\n",
    "    Y5 = Lambda(lambda temp: K.max(temp, axis=1))(x5)\n",
    "    Y6 = Lambda(lambda temp: K.max(temp, axis=1))(x6)\n",
    "    Y7 = Lambda(lambda temp: K.max(temp, axis=1))(x7)\n",
    "    Y8 = Lambda(lambda temp: K.max(temp, axis=1))(x8)\n",
    "\n",
    "    X1 = concatenate([Y1, Y2, Y3, Y4, Y5, Y6, Y7, Y8], axis=-1)\n",
    "\n",
    "    Y1 = Lambda(lambda temp: K.mean(temp, axis=1))(x1)\n",
    "    Y2 = Lambda(lambda temp: K.mean(temp, axis=1))(x2)\n",
    "    Y3 = Lambda(lambda temp: K.mean(temp, axis=1))(x3)\n",
    "    Y4 = Lambda(lambda temp: K.mean(temp, axis=1))(x4)\n",
    "    Y5 = Lambda(lambda temp: K.mean(temp, axis=1))(x5)\n",
    "    Y6 = Lambda(lambda temp: K.mean(temp, axis=1))(x6)\n",
    "    Y7 = Lambda(lambda temp: K.mean(temp, axis=1))(x7)\n",
    "    Y8 = Lambda(lambda temp: K.mean(temp, axis=1))(x8)\n",
    "\n",
    "    X2 = concatenate([Y1, Y2, Y3, Y4, Y5, Y6, Y7, Y8], axis=-1)\n",
    "    X = Concatenate()([X1, X2])\n",
    "\n",
    "    X = Dropout(0.3)(X)\n",
    "    X = WeightNormalization(Dense(class_num, activation='softmax'))(X)\n",
    "\n",
    "    return Model([input], X)\n",
    "\n",
    "\n",
    "proba_t = np.zeros((test['fragment_id'].nunique(), class_num))\n",
    "oof_x = np.zeros((train['fragment_id'].nunique(), class_num))\n",
    "kfold = StratifiedKFold(5, shuffle=True, random_state=2020)\n",
    "\n",
    "for fold, (train_index, valid_index) in enumerate(kfold.split(x, y)):\n",
    "    K.clear_session()\n",
    "    try:\n",
    "        os.system(\"mkdir ./dnn_weights\")\n",
    "    except:\n",
    "        print(\"MKDIR FINISHED...\")\n",
    "    y_ = to_categorical(y, num_classes=class_num)\n",
    "\n",
    "    model = build_model()\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=Adam(1e-3),\n",
    "                  metrics=['acc'])\n",
    "    plateau = ReduceLROnPlateau(monitor=\"val_acc\",\n",
    "                                verbose=0,\n",
    "                                mode='max',\n",
    "                                factor=0.1,\n",
    "                                patience=6)\n",
    "    early_stopping = EarlyStopping(monitor='val_acc',\n",
    "                                   verbose=0,\n",
    "                                   mode='max',\n",
    "                                   patience=10)\n",
    "    checkpoint = ModelCheckpoint(f'./dnn_weights/big-fold{fold}.h5',\n",
    "                                 monitor='val_acc',\n",
    "                                 verbose=0,\n",
    "                                 mode='max',\n",
    "                                 save_best_only=True)\n",
    "    tqdmbar = TQDMProgressBar(show_epoch_progress=False)\n",
    "\n",
    "    model.fit(x[train_index], y_[train_index],\n",
    "              epochs=300,\n",
    "              batch_size=128,\n",
    "              verbose=0,\n",
    "              shuffle=True,\n",
    "              validation_data=(x[valid_index], y_[valid_index]),\n",
    "              callbacks=[plateau, early_stopping, checkpoint, tqdmbar])\n",
    "    model.load_weights(f'./dnn_weights/big-fold{fold}.h5')\n",
    "    oof_x[valid_index] += model.predict(x[valid_index],\n",
    "                                        verbose=0, batch_size=256) / 5.\n",
    "    proba_t += model.predict(t, verbose=0, batch_size=256) / 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-16T06:18:00.110Z"
    }
   },
   "outputs": [],
   "source": [
    "def acc_combo(y, y_pred):\n",
    "    # 数值ID与行为编码的对应关系\n",
    "    mapping = {0: 'A_0', 1: 'A_1', 2: 'A_2', 3: 'A_3', \n",
    "        4: 'D_4', 5: 'A_5', 6: 'B_1',7: 'B_5', \n",
    "        8: 'B_2', 9: 'B_3', 10: 'B_0', 11: 'A_6', \n",
    "        12: 'C_1', 13: 'C_3', 14: 'C_0', 15: 'B_6', \n",
    "        16: 'C_2', 17: 'C_5', 18: 'C_6'}\n",
    "    # 将行为ID转为编码\n",
    "    code_y, code_y_pred = mapping[y], mapping[y_pred]\n",
    "    if code_y == code_y_pred: #编码完全相同得分1.0\n",
    "        return 1.0\n",
    "    elif code_y.split(\"_\")[0] == code_y_pred.split(\"_\")[0]: #编码仅字母部分相同得分1.0/7\n",
    "        return 1.0/7\n",
    "    elif code_y.split(\"_\")[1] == code_y_pred.split(\"_\")[1]: #编码仅数字部分相同得分1.0/3\n",
    "        return 1.0/3\n",
    "    else:\n",
    "        return 0.0\n",
    "    \n",
    "def acc_metric(y, y_pred):\n",
    "    score = 0\n",
    "    for i in range(len(y)):\n",
    "        score += acc_combo(y[i], y_pred[i]) / len(y)\n",
    "    return score\n",
    "\n",
    "target = 'behavior_id'\n",
    "\n",
    "oof_pred = np.argmax(oof_x, axis=1)\n",
    "scores = acc_metric(label[target].values, oof_pred)\n",
    "print(\"NN Offline {}\".format(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
